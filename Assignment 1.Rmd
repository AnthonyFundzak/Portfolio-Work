---
title: "Assignment 1 MIS 64037 Fundzak"
author: "Anthony Fundzak"
date: "3/13/2022"
output:
  word_document: default
  pdf_document: default
---
```
QA1:Regularization tries to optimize the performance on the training set to avoid undertfiting but simultaneously penalize the model when the model becomes too complex so that to avoid overfitting.Another main purpose of regularization when training predictive models is to minimize the loss function and results of the model. 

QA2: The role of the loss function in a predictive model is to evaluate how well the specific algorithm, say MSE, MAE, affects and models the data being worked with. It works with regularization models, such as a lambda, to figure out the optimal measurements for the model to follow and run on.

QA3: In my opinion, you cannot fully trust this model. With a small data set and many hyper parameters, you are constricting the organic functionality and growth of the model. As you set many hyper parameters you risk the chance of unknowingly creating a biased result/algorithm. Hyper parameters are to be used to help estimate the data and not to estimate it to data you think should be the result which in my opinion can happen with the ecessive use of hyper parameters on a small dataset.

QA4: The lambda parameter controls the amount of regularization that is being applied to the model and its data. Lambda balances between minimizing the sum squares of the error on the training set and shirking the  models coefficients. Higher lambda gives heavier weight to reducing model's coefficient or, in other words, heavier regularization causing underfitting, so running different models/algorithms to find the optimal lambda is a necessary step, as you can see below in the subsequent questions. 


> library(ISLR)
> library(dplyr)
> library(glmnet)
> library(caret)
> attach(Carseats)
> summary(Carseats)
     Sales          CompPrice       Income        Advertising       Population        Price      
 Min.   : 0.000   Min.   : 77   Min.   : 21.00   Min.   : 0.000   Min.   : 10.0   Min.   : 24.0  
 1st Qu.: 5.390   1st Qu.:115   1st Qu.: 42.75   1st Qu.: 0.000   1st Qu.:139.0   1st Qu.:100.0  
 Median : 7.490   Median :125   Median : 69.00   Median : 5.000   Median :272.0   Median :117.0  
 Mean   : 7.496   Mean   :125   Mean   : 68.66   Mean   : 6.635   Mean   :264.8   Mean   :115.8  
 3rd Qu.: 9.320   3rd Qu.:135   3rd Qu.: 91.00   3rd Qu.:12.000   3rd Qu.:398.5   3rd Qu.:131.0  
 Max.   :16.270   Max.   :175   Max.   :120.00   Max.   :29.000   Max.   :509.0   Max.   :191.0  
  ShelveLoc        Age          Education    Urban       US     
 Bad   : 96   Min.   :25.00   Min.   :10.0   No :118   No :142  
 Good  : 85   1st Qu.:39.75   1st Qu.:12.0   Yes:282   Yes:258  
 Medium:219   Median :54.50   Median :14.0                      
              Mean   :53.32   Mean   :13.9                      
              3rd Qu.:66.00   3rd Qu.:16.0                      
              Max.   :80.00   Max.   :18.0   
> Carseats_Filtered <- Carseats %>% select("Sales", "Price", "Advertising","Population","Age","Income","Education")

# QB1. Build a Lasso regression model to predict Sales based on all other attributes ("Price", "Advertising", "Population", "Age", "Income" and "Education").  What is the best value of lambda for such a lasso model? 
> y <- Carseats$Sales
> x <- data.matrix(Carseats[, c('Price', 'Advertising', 'Population', 'Age', 'Income', 'Education')])
> preProcess(Carseats, method = c('center', 'scale'))
> as.matrix(Carseats)
> cvfit= cv.glmnet(x,y)
> plot(cvfit)
> cvfit$lambda.min
[1] 0.004305309
> cvfit$lambda.1se
[1] 0.3108781

## The optimal Lambda value for predicting sales based off  of "Price", "Advertising","Population","Age","Income","Education", is cvfit$lambda.min [1] 0.004305309. This value minimizes the cross validation mean square of the error, giving us a high accuracy lambda that predicts the sales based off the said attributes.
{r echo=false} plot(cvfit)

# QB2. What is the coefficient for the price (normalized) attribute in the best model (i.e. model with the optimal lambda)? 
> coef(cvfit, s = "lambda.min")
7 x 1 sparse Matrix of class "dgCMatrix"
                       s1
(Intercept) 15.8945811275
Price       -0.0571800982
Advertising  0.1245132007
Population  -0.0008862575
Age         -0.0486750291
Income       0.0103379764
Education   -0.0347353012

> predict(cvfit, newx = x[1:5,], s = "lambda.min")
  lambda.min
1   8.277828
2   9.895410
3   9.400081
4   8.303545
5   7.008161

## The coefficeient for the price attribute in the best fit model, cvfit, is -0.0571800982, based off the code above. This coefficient gives us the optimal coefficent for the lambda based off predictor Price.

# QB3. How many attributes remain in the model if lambda is set to 0.01? How that number changes if lambda is increased to 0.1? Do you expect more variables to stay in the model (i.e., to have non-zero coefficients) as we increase lambda? 

> coef(cvfit, s = .01)
7 x 1 sparse Matrix of class "dgCMatrix"
                       s1
(Intercept) 15.8120357462
Price       -0.0569053296
Advertising  0.1233400736
Population  -0.0008269647
Age         -0.0482649088
Income       0.0101796053
Education   -0.0324465331
> coef(cvfit, s = .1)
7 x 1 sparse Matrix of class "dgCMatrix"
                      s1
(Intercept) 14.590306998
Price       -0.052573918
Advertising  0.105366130
Population   .          
Age         -0.041822865
Income       0.007643889
Education    . 
> coef(cvfit, s = .5)
7 x 1 sparse Matrix of class "dgCMatrix"
                     s1
(Intercept) 11.84975003
Price       -0.03349181
Advertising  0.04431988
Population   .         
Age         -0.01442737
Income       .         
Education    .  

## From above we can see that when lambda is set to .01, a view variables are heading close to the null section. When we change the lambda to .1 we can see that those same variables are now null and out of the model. For curiousity, if we continue to up the lambda, this time to .5, we see that there are now 3 variables out of the model. This shows us that as we go up increasing lambda, more and more variables will be kicked out of the model. 

#B4. Build an elastic-net model with alpha set to 0.6. What is the best value of lambda for such a model? 

> for (a in seq(0,1,by=0.1)) {cvfit=cv.glmnet(x,y, aplha = a) 
+ print(paste0("alpha is ", a, " and the best MSE ", min(cvfit$cvm)))}
[1] "alpha is 0 and the best MSE 5.126965360247"
[1] "alpha is 0.1 and the best MSE 5.18554513686349"
[1] "alpha is 0.2 and the best MSE 5.10465624337043"
[1] "alpha is 0.3 and the best MSE 5.17083757031995"
[1] "alpha is 0.4 and the best MSE 5.21308069588514"
[1] "alpha is 0.5 and the best MSE 5.19452057033257"
[1] "alpha is 0.6 and the best MSE 5.13467186153194"
[1] "alpha is 0.7 and the best MSE 5.2083940293018"
[1] "alpha is 0.8 and the best MSE 5.14261444146851"
[1] "alpha is 0.9 and the best MSE 5.13571622612068"
[1] "alpha is 1 and the best MSE 5.1597852157499"

##This model has running alpha from 0 to 1 going up by .01 intervals in alpha. So, for alpha = .6, the best value for lambda is 5.13467186153194
